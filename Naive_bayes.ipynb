{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    data = np.genfromtxt(DATA_PATH, dtype=str, delimiter=',')\n",
    "    train_samples_count = int(data.shape[0]*.8)\n",
    "    train = data[:train_samples_count]\n",
    "    test  = data[train_samples_count:]\n",
    "    return train, test\n",
    "#     return train[:,0], train[:,1], test[:,0], test[:,1]\n",
    "\n",
    "# X_train, y_train, X_test, y_test = load_dataset()\n",
    "# print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(894, 2) (224, 2)\n"
     ]
    }
   ],
   "source": [
    "train_samples, test_samples = load_dataset()\n",
    "print(train_samples.shape, test_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.container = dict()\n",
    "        self.prior = defaultdict(int)\n",
    "    \n",
    "        self.unique_words = []\n",
    "    \n",
    "    def initialize_dict_labels(self, labels):\n",
    "        for label in labels:\n",
    "            self.container[label] = defaultdict(int)\n",
    "            \n",
    "    def count_priors(self):\n",
    "        for label in self.data[:,1]:\n",
    "            self.prior[label] += 1\n",
    "\n",
    "        shape = self.data.shape[0]\n",
    "        for key, value in self.prior.items():\n",
    "            self.prior[key] = value/shape\n",
    "            \n",
    "    def fit(self,data):\n",
    "        self.data = np.asarray(data)\n",
    "        self.initialize_dict_labels(self.data[:,1])\n",
    "\n",
    "        for sample in self.data:\n",
    "            words = sample[0].lower().split(' ')\n",
    "            for word in words:\n",
    "                if (word not in self.unique_words): self.unique_words.append(word)\n",
    "                self.container[sample[1]][word] +=1   \n",
    "        \n",
    "        self.count_priors()\n",
    "        \n",
    "    def _predict(self, document):\n",
    "        words = document[0].lower().split(' ')\n",
    "        class_predictions = dict()\n",
    "\n",
    "        for clazz, values in self.container.items():\n",
    "            total_count = sum(values.values())\n",
    "            container = []\n",
    "            \n",
    "            for word in words: \n",
    "                if word in values.keys():\n",
    "                    value = values[word]\n",
    "                else:\n",
    "                    value = 0\n",
    "                \n",
    "                P = (value + 1)/(total_count + len(self.unique_words))\n",
    "                container.append(P)\n",
    "                \n",
    "            class_predictions[clazz] = self.prior[clazz]\n",
    "            for value in container:\n",
    "                class_predictions[clazz] *= value\n",
    "        \n",
    "        return class_predictions\n",
    "\n",
    "    def predict(self, samples):\n",
    "        container = []\n",
    "        for i in samples:\n",
    "            result = self._predict(i[0])\n",
    "            maximum = max(result.items(), key=operator.itemgetter(1))[0]\n",
    "            container.append(maximum == i[1])\n",
    "        return np.asarray(container).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6339285714285714"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = nb.predict(test_samples)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "data = [[\"Chinese Beijing Chinese\",\"0\"],\n",
    "            [\"Chinese Chinese Shanghai\",\"0\"], \n",
    "            [\"Chinese Macao\",\"0\"],\n",
    "            [\"Tokyo Japan Chinese\",\"1\"]]\n",
    "\n",
    "# Fit model\n",
    "s = NaiveBayes()\n",
    "s.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0.00030121377997263036, '1': 0.00013548070246744226}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mpdel predict\n",
    "result = s._predict([\"Chinese Chinese Chinese Tokyo Japan\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must return[ ('Chinese Chinese Chinese Tokyo Japan', '0')]\n",
    "# pobability {'1': 0.00013548070246744226, '0': 0.00030121377997263036}\n",
    "# or log     {'1': -7.906681345001262, '0': -7.10769031284391}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
